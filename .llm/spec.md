# Project Specification: Markdown Link Crawler, Summarizer & DB Storage

## 1. Goal

Create a Node.js ESM CLI tool that:
1.  Extracts all unique HTTP/HTTPS links from markdown files within a specified source directory.
2.  Stores these links in a Supabase Postgres database, tracking their source file and status.
3.  Crawls the web pages corresponding to pending links using a Firecrawl instance.
4.  Saves the scraped markdown content **and metadata (title, description, image URL, keywords)** back to the database.
5.  Generates concise summaries of the crawled markdown content using the OpenAI API (`gpt-4.1-nano`), considering existing tags from the database.
6.  Saves these summaries back to the database.
7.  Manages tags in a separate table and links them to the corresponding links via a junction table.

The tool should be runnable directly from TypeScript source files using Node.js's built-in type stripping.

## 2. Environment & Dependencies

*   **Runtime:** Node.js (Version specified in `.node-version`, supporting direct TS execution and `--watch`).
*   **Environment Variables:** Managed via `direnv` using `.envrc` (gitignored) and `.envrc.example` (template).
    *   `FIRECRAWL_API_KEY`: (Optional) Required if using a non-local/authenticated Firecrawl instance.
    *   `FIRECRAWL_API_URL`: (Optional) Defaults to Firecrawl default if not set.
    *   `OPENAI_API_KEY`: (Required for summarization) Your OpenAI API key.
    *   `SUPABASE_URL`: (Required) URL for your Supabase project's Postgrest API.
    *   `SUPABASE_ANON_KEY`: (Required) The anonymous public key for your Supabase project.
*   **Core Dependencies:**
    *   `glob`: For finding source markdown files.
    *   `remark`, `remark-parse`, `remark-gfm`, `unist-util-visit`: For markdown AST parsing and link extraction.
    *   `@mendable/firecrawl-js`: For interacting with the Firecrawl API.
    *   `openai`: For interacting with the OpenAI API.
    *   `@supabase/supabase-js`: For interacting with the Supabase API.
    *   `p-limit`: For concurrency control.
*   **Dev Dependencies:**
    *   `typescript`, `@types/node`: For type checking.
    *   `esbuild` for bundling
    *   (Optional) `oxlint` for linting.
*   **Built-in Node Modules:**
    *   `node:util`: For `parseArgs`.
    *   `node:fs/promises`: For file reading.
    *   `node:path`: For path manipulation.

## 3. Project Structure

```
.
├── .envrc           # Actual secrets (gitignored)
├── .envrc.example   # Template for env vars
├── .gitignore
├── .node-version    # Node.js version specifier
├── package.json
├── tsconfig.json
├── source-files/    # Input markdown files (user-provided)
│   └── example.md
├── src/
│   ├── index.ts       # Main CLI entry point
│   ├── db.ts          # Database setup and queries
│   ├── supabase-client.ts # Supabase client initialization
│   ├── extract.ts     # Link extraction logic
│   ├── crawl.ts       # Firecrawl logic
│   ├── summarize.ts   # OpenAI summarization logic
│   ├── openai-client.ts # OpenAI API interaction logic
│   └── types.ts       # Shared types (e.g., DB row structure)
└── database.types.ts # Auto-generated Supabase types
```

## 4. Database Schema (`src/db.ts`)

*   **Tool:** `supabase` / Postgres
*   **Management:** Use Supabase CLI for migrations (`supabase db diff --file <name>`, `supabase migration up`).

```sql
-- Schema for the links table
CREATE TABLE public.links (
    id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    url text NOT NULL UNIQUE,
    source_file text NOT NULL,
    -- Metadata from scrape
    title text,
    description text,
    image_url text, -- Populated from og:image
    keywords text[], -- Populated from keywords metadata, if available
    created_at timestamptz DEFAULT CURRENT_TIMESTAMP NOT NULL,
    crawl_status text DEFAULT 'pending'::text NOT NULL CHECK (crawl_status = ANY (ARRAY['pending'::text, 'success'::text, 'failed'::text])),
    crawled_at timestamptz,
    markdown_content text,
    crawl_error text,
    summary_status text DEFAULT 'pending'::text NOT NULL CHECK (summary_status = ANY (ARRAY['pending'::text, 'success'::text, 'failed'::text])),
    summary_error text,
    summary_created_at timestamptz,
    summary_updated_at timestamptz
);

-- Schema for the tags table
CREATE TABLE public.tags (
    id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    name text NOT NULL UNIQUE,
    created_at timestamptz DEFAULT CURRENT_TIMESTAMP NOT NULL
);

-- Schema for the link_tags junction table
CREATE TABLE public.link_tags (
    link_id bigint NOT NULL REFERENCES public.links(id) ON DELETE CASCADE,
    tag_id bigint NOT NULL REFERENCES public.tags(id) ON DELETE CASCADE,
    created_at timestamptz DEFAULT CURRENT_TIMESTAMP NOT NULL,
    PRIMARY KEY (link_id, tag_id)
);

-- Optional: Add indexes for faster lookup of pending items
CREATE INDEX idx_pending_crawl ON public.links (crawl_status) WHERE crawl_status = 'pending';
CREATE INDEX idx_pending_summary ON public.links (crawl_status, summary_status) WHERE crawl_status = 'success' AND summary_status = 'pending';
CREATE INDEX idx_link_tags_link_id ON public.link_tags(link_id);
CREATE INDEX idx_link_tags_tag_id ON public.link_tags(tag_id);
```

*   **Tables:**
    *   `links`:
        *   `id`, `url`, `source_file`, `created_at`, `crawl_status`, `crawled_at`, `markdown_content`, `crawl_error`, `summary_status`, `summary`, `summary_error`, `summary_created_at`, `summary_updated_at`
        *   **New/Updated:** `title`, `description`, `image_url`, `keywords` (populated from scrape)
    *   `tags`:
        *   `id`, `name` (unique), `created_at`
    *   `link_tags` (Junction Table):
        *   `link_id` (FK -> links.id), `tag_id` (FK -> tags.id), `created_at`, PRIMARY KEY (`link_id`, `tag_id`)
*   **Functions/Queries (Conceptual):**
    *   `initDb()`: (No longer needed for schema creation; handled by Supabase migrations). Establishes Supabase client connection.
    *   `addLinks(links: {url: string, sourceFile: string}[])`: Batch insert links using Supabase client, handling potential conflicts on `url`.
    *   `getPendingLinks(limit: number)`: Fetch links where `crawl_status = 'pending'`.
    *   `updateLinkCrawlSuccess(id: number, data: LinkUpdate)`: Update status, markdown content, metadata fields, timestamp for successful crawl.
    *   `updateLinkCrawlFailure(id: number, error: string)`: Update status, error, timestamp for failed crawl.
    *   `getAllTags()`: Fetch all existing tags (`id`, `name`).
    *   `getPendingSummaries(limit: number)`: Fetch links (`id`, `url`, `markdown_content`, `title`, `description`) where `crawl_status = 'success'` AND `summary_status = 'pending'`.
    *   `upsertTags(tags: {name: string}[])`: Upsert tags based on name, returning their `id` and `name`.
    *   `updateLinkSummarySuccess(id: number, summary: string)`: Update summary text, status, timestamp.
    *   `deleteLinkTags(linkId: number)`: Delete existing tag associations for a link.
    *   `insertLinkTags(associations: {link_id: number, tag_id: number}[])`: Insert new link-tag associations.
    *   `updateLinkSummaryFailure(id: number, error: string)`: Update status, error, timestamp for failed summary.

## 5. CLI Interface (`src/cli.ts`)

*   **Parsing:** Use `node:util`'s `parseArgs`.
*   **Arguments/Options:**
    *   `--source-dir <path>` (`-s`): Path to source markdown files. Default: `./source-files`.
    *   `--supabase-url <url>` (`-u`): Supabase project URL. Default: Env var `SUPABASE_URL`.
    *   (Note: `SUPABASE_ANON_KEY` is required via environment variable only)
    *   `--firecrawl-api-url <url>`: Firecrawl API URL. Default: Env var `FIRECRAWL_API_URL` or library default.
    *   `--concurrency <number>`: Max concurrent crawls/summaries. Default: `5`.
    *   `--skip-extraction`: Flag to skip finding and adding new links. Default: `false`.
    *   `--skip-crawl`: Flag to skip crawling pending links. Default: `false`.
    *   `--skip-summary`: Flag to skip summarizing crawled content. Default: `false`.
    *   `--reset-db`: Flag to delete and recreate the database before starting. Default: `false`.
    *   `--help` (`-h`): Display help message.
*   **Execution Flow:**
    1.  Parse args using `parseArgs`. Handle `--help`.
    2.  Validate required args/env vars (e.g., `OPENAI_API_KEY` if needed).
    3.  Initialize DB connection (`db.initDb`), handle `--reset-db`.
    4.  If `!skip-extraction`: Run link extraction (`extract.run`).
    5.  If `!skip-crawl`: Run crawling (`crawl.run`).
    6.  If `!skip-summary`: Run summarization (`summarize.run`).
    7.  Log progress and completion status.

## 6. Link Extraction (`src/extract.ts`)

*   Input: `sourceDir`, `dbInstance`.
*   Use `glob` to find `**/*.md` in `sourceDir`.
*   For each file:
    *   Read content using `fs.readFile`.
    *   Parse using `remark().use(remarkGfm).parse()`.
    *   Visit `link` nodes using `unist-util-visit`.
    *   Filter for valid `http://` or `https://` URLs.
    *   Collect unique `{ url, sourceFile }` pairs (relative path for `sourceFile`).
*   Batch insert collected links using `db.addLinks`.

## 7. Crawling (`src/crawl.ts`)

*   Input: `dbInstance`, `firecrawlApiKey` (optional), `firecrawlApiUrl`, `concurrency`.
*   Initialize `FirecrawlApp`.
*   Loop:
    *   Fetch batch of pending links (`db.getPendingLinks`). Exit loop if empty.
    *   Process batch concurrently using `p-limit` and `Promise.allSettled`.
    *   For each link:
        *   Call `firecrawlApp.scrapeUrl(link.url, { formats: ['markdown'] })`.
        *   On success: Extract markdown and metadata. Call `db.updateLinkCrawlSuccess` with all relevant data (`markdown_content`, `title`, `description`, `image_url`, `keywords`, etc.).
        *   On failure: `db.updateLinkCrawlFailure`.
    *   Handle potential rate limiting/errors.

## 8. Summarization (`src/summarize.ts`)

*   Input: `dbInstance`, `concurrency`.
*   Fetch all existing tags from the `tags` table once (`db.getAllTags`).
*   Loop:
    *   Fetch batch of pending summaries (`db.getPendingSummaries`). Exit loop if empty.
    *   Process batch concurrently using `p-limit` and `Promise.allSettled`.
    *   For each record:
        *   Call `callOpenAI` (from `src/openai-client.ts`) passing content, url, title, description, and the list of existing tag names.
        *   On OpenAI success:
            *   Upsert the tags returned by OpenAI into the `tags` table (`db.upsertTags`), getting their IDs.
            *   Delete existing associations for the link in `link_tags` (`db.deleteLinkTags`).
            *   Insert new associations into `link_tags` using the link ID and the upserted tag IDs (`db.insertLinkTags`).
            *   Update the `links` table with the summary text and success status (`db.updateLinkSummarySuccess`).
        *   On OpenAI failure: Update the `links` table with error status (`db.updateLinkSummaryFailure`).
*   Handles potential OpenAI and Database errors.

*   **`src/openai-client.ts` Details:**
    *   `callOpenAI(markdownContent, sourceUrl, pageTitle, pageDescription, existingTagNames)`:
        *   Initializes the OpenAI client using the `OPENAI_API_KEY` environment variable.
    *   Uses the `openai` library to call the Chat Completions API.
    *   Uses the model `gpt-4.1-nano`.
    *   Sends a structured prompt requesting a JSON object containing `summary` and `tags`, including the existing tags list and instructions to prioritize them and limit new tags.
    *   Handles API errors and retries.
    *   Parses the JSON response and returns an object `{ summary: string, tags: string[] }`.

## 9. `package.json` Scripts

*   `start`: `node src/index.ts [args]` - Run the CLI.
*   `dev`: `node --watch src/index.ts [args]` - Run with auto-reload on changes.
*   `typecheck`: `tsc --noEmit` - Perform static type checking.
*   `test`: `node --test` - Run unit/integration tests.

## 10. Testing

*   **Framework:** Use Node.js's built-in test runner (`node:test`).
*   **Strategy:**
    *   **Unit Tests:** Create test files (e.g., `src/db.test.ts`, `src/extract.test.ts`) for individual modules.
        *   Mock external dependencies:
            *   Use an in-memory SQLite database for `db.ts` tests.
            *   Mock `node:fs/promises` for `extract.ts` tests.
            *   Mock `fetch` calls to Firecrawl API for `crawl.ts` tests.
            *   Mock the `openai` library calls for `summarize.ts` tests.
            *   Mock Supabase client calls for database interactions.
    *   **Integration Tests:** (Optional) Could test the overall CLI flow by mocking external APIs and verifying database state changes.
*   **Location:** Test files should ideally live alongside the code they test (e.g., `src/db.test.ts`) or in a separate `test/` directory.
*   **Execution:** Run tests using `npm test` or `node --test`.

## 11. Future Considerations / Enhancements (Optional)

*   More sophisticated error handling and retry logic (especially for network requests).
*   Batching database updates for potentially better performance.
*   Allowing custom summarization prompts via CLI flags.
*   Support for other source file types (e.g., HTML).
*   Progress indicators (e.g., using a library like `ora`).
